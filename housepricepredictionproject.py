# -*- coding: utf-8 -*-
"""Housepricepredictionproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13D04ZqQjNNmAxIuSOSWvO4d_ZMXB6lD4

*comments are mentioned always for the line below*

**median_house_value** is target column
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io

"""LOADING DATASET :-"""

from google.colab import files

uploaded = files.upload()

# reading the csv file
data = pd.read_csv('housing.csv',sep=',')

# printing the first few datavalues from data
print(data.head())

"""DATA EXPLORATION :-"""

# to check for existence of null value
data.info() #result shows presence of null values in total_bedrooms

# removes the rows containing null values
# inplace = True will make direct changes in original datset
data.dropna(inplace=True)

data.info() # we can see now changes has been made

# splitting dataset into train and test values
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# X is whole df without 'median_house_value'
X = data.drop(columns=['median_house_value'],axis=1)
# y = target column = 'median_house_value'
y= data['median_house_value']

print(y)  # just checking

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)
y_train
X_train

# correlating the X and y data
# train_data = X_train.join(y_train) # Add suffixes to distinguish overlapping columns
# print(train_data)


train_data = X.copy()
train_data['median_house_value'] = y

# visualizing by histogram
train_data.hist(figsize=(15,10))

# Convert 'ocean_proximity' to numerical data using one-hot encoding i.e., True or False
# train_data = pd.get_dummies(train_data, columns=['ocean_proximity'])
label_encoder = LabelEncoder()

data['ocean_proximity_encoded'] = label_encoder.fit_transform(data['ocean_proximity'])

X = data.drop(['ocean_proximity'], axis=1)

print(X)

# Calculate the correlation matrix correlating values within column
# train_data.corr()


# THIS CORRELATED DATA HELPS THE BEST TO CHOOSE THE FACTORS TO WORK UPON
# visulaizing by heatmap
# annotaion = True to see the actual correlation numbers ; colormap =  yellowgreenblue
plt.figure(figsize=(15,10))
sns.heatmap(train_data.corr(),annot=True,cmap='YlGnBu')
plt.show()

"""DATA PREPROCESSING :-"""

# we are doing this for the graphs which are seen as skewed(unsymmetric on left and right sides of median)
# +1 is done to avoid issues when taking logarithm of values closer to 0
# np.log is taken so that now date is more normally distributed to avoid impact of outliers
train_data['total_rooms']=np.log(train_data['total_rooms']+1)
train_data['total_bedrooms']=np.log(train_data['total_bedrooms']+1)
train_data['population']=np.log(train_data['population']+1)
train_data['households']=np.log(train_data['households']+1)
# skewness will be gone now

# train_data.hist(figsize=(15,8))

# # Access one of the new one-hot encoded columns
# # Example: if one of the new columns is 'ocean_proximity_<1H OCEAN'
# print(train_data['ocean_proximity_<1H OCEAN'].value_counts())
# print(train_data['ocean_proximity_INLAND'].value_counts())
# print(train_data['ocean_proximity_NEAR OCEAN'].value_counts())
# print(train_data['ocean_proximity_NEAR BAY'].value_counts())
# print(train_data['ocean_proximity_ISLAND'].value_counts())
# # pd.get_dummies:- converts categorical variables into one-hot encoded binary columns; & it is already done in above lines in behined



# --------label encoder proved better than pd.get_dummies-------------

train_data
# here we can confirm that role of [pd.get_dummies(not used)] label_encoder(used) was practised in above box while value_counts()

# now lets check how the score relates to each of the subbranch of ocean_proximity
plt.figure(figsize=(15,10))
sns.heatmap(train_data.corr(),annot=True,cmap='YlGnBu')
# the heatmap thus created shows that -ve values relation directs to the less price for median house value and +ve values directs to higher price

# this step is just to get the insight lower lattitude and longitude directs to near ocean so if we are towards ocean then prices are predicted high
sns.scatterplot(data=train_data,x='longitude',y='latitude',hue='median_house_value',palette='viridis')

"""FEATURE ENGINEERING :-

This step involves finding different featues and how they can be used to identify any specific property which can be helpful for applying ml for prediction.
This is just to keep exploring knowledge, finding/researching factors that can help kind of step
"""

# no. of bedrooms per no. of rooms can be useful feature
train_data['bedroom_per_room'] = train_data['total_bedrooms']/train_data['total_rooms']
train_data

plt.plot(train_data['bedroom_per_room'],train_data['median_house_value'],marker='o')
plt.xlabel('Bedrooms per Room')
plt.ylabel('Median House Value')
plt.title('Relationship between Bedrooms per Room and Median House Value')
plt.show()
# this graph shows that almost for all median_house_value's bedroom_per_room is between 0.7 to 0.8 almost everytime

plt.figure(figsize=(15,10))
sns.heatmap(train_data.corr(),annot=True,cmap='YlGnBu')
# so this feature was not feasible for the research as relation has -ve value
# which confirms the observation in precious code block

"""Do regression and not classification because of continuous values

Linear Regression:-

by using MinMaxScaler
"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    train_data.drop(['median_house_value'], axis=1),
    train_data['median_house_value'],
    test_size=0.2,
    random_state=42
)

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit the scaler on the training data and transform both training and test data
scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test = scaler.transform(X_test)

# Initialize the Linear Regression model
model = LinearRegression()

# Train the model on the scaled training data
model.fit(scaled_X_train, y_train)

# Evaluate the model on the scaled test data
train_score = model.score(scaled_X_train, y_train)
test_score = model.score(scaled_X_test, y_test)

print(f"Training Score: {train_score}")
print(f"Test Score: {test_score}")

"""by using StandardScaler"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Splitting the data into training and testing sets
X = train_data.drop(['median_house_value'], axis=1)
y = train_data['median_house_value']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform it
scaled_X_train = scaler.fit_transform(X_train)

# Scale the test data using the same scaler
scaled_X_test = scaler.transform(X_test)

# Initialize the model
model = LinearRegression()

# Train the model on the scaled training data
model.fit(scaled_X_train, y_train)

# Evaluate the model on the scaled test data
score = model.score(scaled_X_test, y_test)
print(f"Model Score: {score}")

"""below code is not useful now as using label_encoder in place of pd.get_dummies"""

# test_data = X_test.join(y_test)

# # Apply log transformation to relevant columns
# for col in ['total_rooms', 'total_bedrooms', 'population', 'households']:
#     test_data[col] = np.log1p(test_data[col])

# # Check if 'ocean_proximity' exists and perform one-hot encoding
# if 'ocean_proximity' in test_data.columns:
#     test_data = pd.get_dummies(test_data, columns=['ocean_proximity'], drop_first=True)
# else:
#     print("Warning: 'ocean_proximity' column not found in test_data.")


# test_data['bedroom_per_room'] = test_data['total_bedrooms'] / test_data['total_rooms']

# # Separate the features and target variable
# X_test = test_data.drop('median_house_value', axis=1)
# y_test = test_data['median_house_value']

# score = model.score(X_test, y_test)
# print(f"Model Score: {score}")

"""Random Forest Regressor:-"""

from sklearn.ensemble import RandomForestRegressor
forest = RandomForestRegressor()
forest.fit(scaled_X_train, y_train)
forest_score = forest.score(scaled_X_test, y_test)
print(f"Model Score: {forest_score}")

from sklearn.model_selection import GridSearchCV

# parameter_grid will cross validate all the features
parameter_grid = {
    "n_estimators" : [3,50,100],
    "max_features" : [2,4,6,8],
    "min_samples_split" : [2,4,6]
}
# cv = cross-validation i.e., making several combinations out of the hyperparameters in the parameter_grid
grid_search = GridSearchCV(forest,parameter_grid, cv=5,scoring='neg_mean_squared_error',return_train_score=True)
grid_search.fit(scaled_X_train,y_train)

# grid_search.best_params_
grid_search.best_estimator_
grid_search.best_estimator.score(scaled_X_test,y_test)

